# ğŸ§ª Scripts Usage Guide

## Article Topic Classification -- Command Reference

This document describes how to run each script in the `scripts/`
directory.

All commands should be executed from the project root directory.

------------------------------------------------------------------------

# ğŸ“ Available Scripts

    scripts/
    â”œâ”€â”€ data_sanity_check.py
    â”œâ”€â”€ split_dataset.py
    â”œâ”€â”€ train_distilbert.py
    â”œâ”€â”€ evaluate_distilbert.py
    â”œâ”€â”€ calibration_analysis.py
    â”œâ”€â”€ predict.py
    â””â”€â”€ batch_predict.py

------------------------------------------------------------------------

# ğŸ” 1ï¸âƒ£ Data Sanity Check

``` bash
python -m scripts.data_sanity_check
```

Verifies:

-   Total sample count\
-   Label distribution\
-   Token length statistics\
-   Dataset integrity

âœ” Ensures clean and balanced input data before training.

------------------------------------------------------------------------

# âœ‚ï¸ 2ï¸âƒ£ Train / Validation / Test Split

``` bash
python -m scripts.split_dataset
```

Creates:

-   `train.jsonl`\
-   `val.jsonl`\
-   `test.jsonl`

âœ” Maintains label distribution\
âœ” Prevents data leakage

------------------------------------------------------------------------

# ğŸ§  3ï¸âƒ£ Model Training (DistilBERT Fine-Tuning)

``` bash
python -m scripts.train_distilbert
```

What happens:

-   Loads training and validation datasets\
-   Fine-tunes DistilBERT\
-   Uses GPU automatically if available\
-   Applies AdamW optimizer + linear scheduler\
-   Saves model to `artifacts/distilbert/`\
-   Logs training progress

âœ” Typical convergence: loss \~1.2 â†’ \~0.14

------------------------------------------------------------------------

# ğŸ“Š 4ï¸âƒ£ Model Evaluation (Unseen Test Set)

``` bash
python -m scripts.evaluate_distilbert
```

Reports:

-   Accuracy (\~90%)\
-   Precision / Recall / F1-score\
-   Confusion matrix\
-   Confidence statistics\
-   Expected Calibration Error (ECE)\
-   Coverage vs Accuracy curve

âœ” Strict evaluation on `test.jsonl` only\
âœ” Validates confidence-aware routing thresholds

------------------------------------------------------------------------

# ğŸ“ˆ 5ï¸âƒ£ Calibration & Selective Classification Analysis

``` bash
python -m scripts.calibration_analysis
```

Computes:

-   Expected Calibration Error (ECE)\
-   Reliability diagnostics\
-   Coverage vs Accuracy tradeoff

âœ” Used to validate production confidence threshold (\~0.85)\
âœ” Enables risk-aware deployment

------------------------------------------------------------------------

# ğŸ§ª 6ï¸âƒ£ Single Article Inference

``` bash
python -m scripts.predict
```

Returns structured output:

``` json
{
  "prediction": "World",
  "confidence": 0.97,
  "all_probabilities": {
    "World": 0.97,
    "Sports": 0.01,
    "Business": 0.01,
    "Sci/Tech": 0.01
  },
  "decision": "auto_accept",
  "top2_label": "Business",
  "top2_gap": 0.94
}
```

âœ” Applies confidence-aware decision routing

------------------------------------------------------------------------

# ğŸ“¦ 7ï¸âƒ£ Batch Inference

``` bash
python -m scripts.batch_predict
```

-   Reads input JSONL file\
-   Loads model once\
-   Processes articles using DataLoader batching\
-   Writes output JSONL with predictions

âœ” Efficient for large-scale processing\
âœ” Mimics real-world batch ML pipelines

------------------------------------------------------------------------

# ğŸ Summary

These scripts collectively demonstrate:

-   Clean ML lifecycle management\
-   Proper dataset handling\
-   Rigorous evaluation\
-   Calibration & selective classification\
-   Real-time and batch inference patterns

They form the backbone of the production-ready NLP pipeline.
