# Article Topic Classifier - Complete Architecture Guide

## üìã Project Overview

This project implements a **fine-tuned DistilBERT model** that classifies news articles into 4 topics:
- üåç **World** - International news
- üèÜ **Sports** - Sports articles
- üíº **Business** - Business and finance
- üî¨ **Sci/Tech** - Science and technology

The model uses **transfer learning**: starting from a pre-trained DistilBERT model and fine-tuning it on our specific task.

---

## üèóÔ∏è Project Structure

```
article-topic-classifier/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ train.yaml              # Hyperparameters and settings
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ part-0001.jsonl         # Training articles (JSON Lines format)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ data_sanity_check.py    # Validate data before training
‚îÇ   ‚îî‚îÄ‚îÄ train_distilbert.py     # Main training script
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py              # Custom dataset class
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py              # Evaluation metrics
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                # Logging configuration
‚îú‚îÄ‚îÄ artifacts/
‚îÇ   ‚îî‚îÄ‚îÄ distilbert/             # Saved model and tokenizer
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ [analysis and visualization notebooks]
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ training_*.log          # Training logs
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îî‚îÄ‚îÄ README.md                   # Project documentation
```

---

## üîÑ How Everything Works Together

### 1Ô∏è‚É£ **Configuration (config/train.yaml)**

```yaml
model:
  name: distilbert-base-uncased  # Pre-trained model from Hugging Face
  num_labels: 4                  # 4 topic classes

training:
  epochs: 3                      # Train for 3 full passes through data
  batch_size: 16                 # Process 16 articles at a time
  learning_rate: 0.00002         # How fast to update weights
  max_length: 256                # Truncate/pad articles to 256 tokens
  weight_decay: 0.01             # L2 regularization (prevent overfitting)
  warmup_ratio: 0.1              # Warmup for first 10% of training
```

**Why these values?**
- **DistilBERT**: 40% smaller and 60% faster than BERT while maintaining 97% of performance
- **Learning rate (0.00002)**: Very small because we're fine-tuning (weights already good)
- **Batch size (16)**: Good balance between speed and memory usage
- **Max length (256)**: Most articles fit comfortably; more tokens = more computation

---

## üìä Data Pipeline

### Step 1: Raw Data ‚Üí Dataset Class

**Input:** `data/part-0001.jsonl` (articles in JSON Lines format)

```json
{"title": "Breaking News", "body": "Full article text...", "topic": "World"}
{"title": "Sports Update", "body": "Game highlights...", "topic": "Sports"}
```

### Step 2: Tokenization

**Process:** `NewsDataset` class converts text ‚Üí token IDs

```python
Text: "Breaking News. Full article"
         ‚Üì Tokenizer
Tokens: [101, 8149, 1038, 1012, 2440, 3720, 102, 0, 0...]
         ‚Üë                                           ‚Üë
       [CLS] token                                 [PAD] tokens
```

**Key Components:**
- **[CLS]** token: Special token at start (used for classification)
- **[SEP]** token: Separates sentences
- **Subword tokens**: "Breaking" ‚Üí "Breaking", but "tokenization" ‚Üí "token", "##ization"
- **[PAD]** tokens: Fill sequences shorter than max_length

### Step 3: Attention Mask

Tells model which tokens are real vs padding:
```
Tokens:          [101, 8149, 1012, 2440, 0, 0]
Attention Mask:  [1,   1,    1,    1,    0, 0]
                 ‚Üë real tokens ‚Üë  ‚Üë padding ‚Üë
```

---

## üß† Model Architecture

### DistilBERT Structure

```
Input Text
    ‚Üì
Tokenizer ‚Üí Token IDs [101, 8149, 1012, ...]
    ‚Üì
Embedding Layer (converts IDs ‚Üí 768-dimensional vectors)
    ‚Üì
6 Transformer Layers (each with 12 attention heads)
    ‚Üì
[CLS] Token Output (768-dimensional vector representing whole text)
    ‚Üì
Classification Head (768 ‚Üí 4 classes)
    ‚Üì
Output: Probabilities for each topic
    ‚Üì
Softmax: [0.05, 0.02, 0.88, 0.05]
         World Sports Business Sci/Tech
```

**Why [CLS] token?** It's a learned representation of the entire text (summary). Perfect for classification!

---

## üîß Training Process

### Training Loop Overview

```
For each Epoch:
    For each Batch (16 articles):
        1. Tokenize articles
        2. Forward pass ‚Üí Get predictions
        3. Calculate loss (how wrong we are)
        4. Backward pass ‚Üí Calculate gradients
        5. Update weights using optimizer
        6. Update learning rate using scheduler
        7. Log progress
```

### Detailed Step-by-Step

#### Step 1: Forward Pass
```python
output = model(
    input_ids=token_ids,           # Tokenized text
    attention_mask=attention_mask,  # Tell model what's real vs padding
    labels=topic_ids                # True labels (0-3)
)
loss = output.loss  # Cross-entropy loss
```

Cross-entropy loss measures how different predicted probabilities are from true labels:
- Perfect prediction [0, 0, 1, 0] ‚Üí loss = 0
- Bad prediction [0.25, 0.25, 0.25, 0.25] ‚Üí loss = high

#### Step 2: Backward Pass (Gradient Calculation)
```python
loss.backward()  # Compute ‚àÇloss/‚àÇweights for all layers
```

This tells us: "To reduce loss, change weight W by gradient G"

#### Step 3: Parameter Update (Gradient Descent)
```python
optimizer.step()  # weights = weights - (learning_rate √ó gradient)
```

Example: If gradient = 0.001, learning_rate = 0.00002
- New weight = old weight - (0.00002 √ó 0.001) = old weight - 0.00000002

#### Step 4: Learning Rate Scheduling
```python
scheduler.step()  # Adjust learning rate
```

**Warmup Phase (first 10%):**
- Learning rate: 0 ‚Üí 0.00002 gradually
- Why? Prevent instability with pre-trained weights

**Decay Phase (remaining 90%):**
- Learning rate: 0.00002 ‚Üí 0 gradually
- Why? Fine-grained updates as we approach convergence

---

## üìù Key Training Concepts

### 1. **Fine-tuning vs Training from Scratch**

| Aspect | From Scratch | Fine-tuning |
|--------|-------------|------------|
| Learning rate | 0.001-0.01 (large) | 0.00002 (tiny) |
| Epochs | 10-100 | 2-5 |
| Data needed | 100K+ examples | 500+ examples |
| Time | Days/weeks | Minutes/hours |
| Performance | Often worse | Usually better |

We use **fine-tuning** because:
- DistilBERT already learned language patterns
- We only need to teach it our specific task
- Small learning rate prevents "forgetting" pre-trained knowledge

### 2. **Batch Processing**

Why batch 16 articles instead of 1?
- **Speed**: GPU can process 16 in parallel
- **Stability**: Gradients from 16 samples more reliable than 1
- **Memory**: 16 √ó 256 tokens ‚âà 4KB per batch (GPU has 24GB)

### 3. **Loss Function**

We use **Cross-Entropy Loss** (standard for classification):

```
Loss = -Œ£(true_label √ó log(predicted_probability))

Example:
- True label: Business [0, 0, 1, 0]
- Prediction: [0.05, 0.02, 0.88, 0.05]
- Loss = -log(0.88) ‚âà 0.128 (lower is better)
```

---

## üì¶ Dependencies & Environment

### Required Libraries

```
torch                    # Deep learning framework
transformers>=4.0       # Pre-trained models (BERT, DistilBERT, etc.)
pandas                  # Data manipulation
PyYAML                  # Load config files
scikit-learn            # Evaluation metrics
```

### Why Each?
- **torch**: PyTorch framework for neural networks
- **transformers**: Hugging Face library with pre-trained models
- **pandas**: Handle data in DataFrames
- **PyYAML**: Parse YAML config files (human-readable settings)
- **scikit-learn**: Precision, recall, F1-score calculations

---

## üìä Monitoring Training Progress

### Log Files

Training creates logs in two places:

1. **File Logs** (`logs/training_20260208_143022.log`)
   - Detailed: includes timestamps, module names, debug info
   - Used for debugging issues

2. **Console Output**
   - Summary only: key milestones
   - Used for real-time progress monitoring

### Example Log Output

```
INFO - ================================================================================
INFO - Starting training pipeline
INFO - ================================================================================
INFO - Loading configuration from config/train.yaml
INFO - Config loaded successfully
INFO - Using device: cuda
INFO - Loading tokenizer: distilbert-base-uncased
INFO - Tokenizer loaded successfully
INFO - Loading model: distilbert-base-uncased with 4 labels
INFO - Model moved to cuda
INFO - Loading dataset from data/part-0001.jsonl
INFO - Dataset loaded: 500 samples
INFO - Creating DataLoader with batch_size=16
INFO - DataLoader created: 32 batches

INFO - Starting training for 3 epochs
INFO - 
INFO - Epoch 1/3
INFO -   Batch 6/31 | Loss: 1.3245
INFO -   Batch 12/31 | Loss: 1.1234
INFO -   Batch 19/31 | Loss: 0.9834
INFO -   Batch 25/31 | Loss: 0.8567
INFO -   Batch 31/31 | Loss: 0.7834
INFO - Epoch 1 completed | Avg Loss: 0.9456
...
INFO - Training completed! Saving model to artifacts/distilbert
INFO - ‚úÖ Model and tokenizer saved successfully
```

---

## üíæ Output Files

### After Training

```
artifacts/distilbert/
‚îú‚îÄ‚îÄ config.json              # Model architecture configuration
‚îú‚îÄ‚îÄ pytorch_model.bin        # Model weights (268 MB)
‚îú‚îÄ‚îÄ tokenizer.json           # Tokenizer vocabulary and rules
‚îú‚îÄ‚îÄ tokenizer_config.json    # Tokenizer settings
‚îî‚îÄ‚îÄ vocab.txt                # List of all tokens
```

These files are all you need to make predictions on new articles!

---

## üîÆ How to Use the Trained Model

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load saved model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("artifacts/distilbert")
model = AutoModelForSequenceClassification.from_pretrained("artifacts/distilbert")

# Predict topic for new article
text = "Breaking news from the tech industry..."
inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
outputs = model(**inputs)

# Get predicted class
predicted_class = outputs.logits.argmax(dim=-1).item()
topics = ["World", "Sports", "Business", "Sci/Tech"]
print(f"Topic: {topics[predicted_class]}")
```

---

## üêõ Troubleshooting

### Issue: Out of Memory (OOM)
- **Cause**: Batch size too large for GPU
- **Solution**: Reduce `batch_size` in config (8, 4)

### Issue: Loss not decreasing
- **Cause**: Learning rate wrong
- **Solution**: Try 0.00001 or 0.00005

### Issue: Model predicts same class for everything
- **Cause**: Insufficient training data or epochs
- **Solution**: Increase `epochs` to 5-10, add more data

---

## üìö Learning Resources

### Understanding Transformers
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Original transformer paper
- [Hugging Face Course](https://huggingface.co/course) - Interactive learning

### Transfer Learning
- [Fine-tuning BERT](https://huggingface.co/docs/transformers/training) - Official guide

### PyTorch
- [PyTorch Documentation](https://pytorch.org/docs) - Complete reference

---

## ‚úÖ Next Steps

1. **Run Training**: `python -m scripts.train_distilbert`
2. **Monitor Progress**: Check `logs/` directory for detailed logs
3. **Evaluate**: Create evaluation script using `src/metrics.py`
4. **Deploy**: Use saved model for inference on new articles
5. **Improve**: Experiment with hyperparameters, more data, different architectures

---

## üéØ Summary

This project demonstrates:
- ‚úÖ Transfer learning with pre-trained models
- ‚úÖ Fine-tuning for custom tasks
- ‚úÖ Proper data handling and preprocessing
- ‚úÖ Production-quality logging and monitoring
- ‚úÖ Clean, commented, readable code

The architecture is modular and extensible - you can easily swap DistilBERT for BERT, RoBERTa, or other models from Hugging Face!
